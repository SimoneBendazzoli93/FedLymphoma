{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedLymphoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Federation using NVFlare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"<YOUR_USERNAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
    "\n",
    "sess = new_secure_session(\n",
    "    username,\n",
    "    f\"StartupKit/{username}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = sess.list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.abort_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Preparation\n",
    "\n",
    "The first step in the Federated Learning process is to prepare the job configuration files. The job configuration files contain the necessary information to run the job on the Federated Learning cluster, such as the job type, the resources required, and the parameters for the job execution.\n",
    "\n",
    "The job configurations files are automatically generated by the script `nvflare_generate_job_configs`, which is installed together with this package. The script takes as input client-specific configuration, together with the experiment-specific configuration, and generates the job configuration files for each client.\n",
    "\n",
    "\n",
    "### Client Configuration\n",
    "The client-specific configuration file should be in the following format:\n",
    "\n",
    "```yaml\n",
    "data_dir: \"<DATASET_FOLDER>\"\n",
    "modality_dict:\n",
    "  ct: \"<CT_SUFFIX>\"\n",
    "  label: \"<SEG_MASK_SUFFIX>\"\n",
    "dataset_format: \"<DATASET_FORMAT>\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"<NNUNET_ROOT_FOLDER>\"\n",
    "client_name: \"<CLIENT_NAME>\"\n",
    "subfolder_suffix: \"<SUBFOLDER_SUFFIX>[OPTIONAL]\"\n",
    "bundle_root: \"<BUNDLE_ROOT>[OPTIONAL]\"\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "`dataset_format` should refer to one of  these three different formats, according to the `data_dir` structure:\n",
    "1. `subfolders`: The dataset is organized in subfolders, where each subfolder corresponds to a subject and contains the images and labels for that subject.\n",
    "```plaintext\n",
    "  [Dataset_folder]\n",
    "        [Subject_0]\n",
    "            - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "            - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "            - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        [Subject_1]\n",
    "            - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "            - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "            - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "2. `decathlon`: The dataset is organized in the format of the Medical Decathlon challenge, where the images and labels are stored in separate folders.\n",
    "```plaintext\n",
    "  [Dataset_folder]\n",
    "        [imagesTr]\n",
    "            - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "            - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "            - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "            - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "        [labelsTr]\n",
    "            - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "            - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "\n",
    "3. `nnunet`: The dataset has been already prepared according to the nnUNet framework, with the images and labels stored in separate folders.\n",
    "```plaintext\n",
    "  [nnUNet_raw]\n",
    "      [DatasetXYZ_TaskName]  # THIS IS THE DATASET FOLDER\n",
    "          dataset.json\n",
    "          [imagesTr]\n",
    "              - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "              - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "              - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "              - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "          [labelsTr]\n",
    "              - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "              - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "\n",
    "`nnunet_root_folder` should refer to the root folder used by the nnUnet framework, where the nnUNet experiments are stored.\n",
    "For the `subfolders` and `decathlon` dataset formats, this folder is created during the dataset preparation step. \n",
    "For the `nnunet` dataset format, this folder should contain the nnUNet experiments, with the following structure:\n",
    "```plaintext\n",
    "  [nnunet_root_folder]\n",
    "      [nnUNet_raw_data_base]\n",
    "          [DatasetXYZ_TaskName]\n",
    "              dataset.json\n",
    "              [imagesTr]\n",
    "                  - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "                  - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "                  - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "                  - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "              [labelsTr]\n",
    "                  - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "                  - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "            ...\n",
    "\n",
    "```\n",
    "\n",
    "`modality_dict` is a dictionary that maps the modality names to the file suffixes. The suffixes are used to identify the files that correspond to the different modalities in the dataset. For example, if the CT images have the suffix `_CT.nii.gz`, the entry in the `modality_dict` should be `ct: \"_CT.nii.gz\"`.\n",
    "\n",
    "\n",
    "`patient_id_in_file_identifier` is a flag used to specify if the patient ID is included in the file name. If this flag is set to `True`, the patient ID will be extracted from the file name. If this flag is set to `False`, the patient ID will be extracted from the file path. If set to `False`, the filename should only contain the modality suffix.\n",
    "\n",
    "`client_name` is a unique identifier for the client.\n",
    "\n",
    "`subfolder_suffix` is an optional parameter that specifies the suffix of the subfolders that contain the images and labels for each subject. This parameter is used when the dataset is organized in subfolders, and the subfolders have a specific suffix that needs to be removed to extract the patient ID.\n",
    "\n",
    "`bundle_root` is an optional parameter that specifies the root folder where the MONAI Bundle is stored.\n",
    "\n",
    "### Experiment Configuration\n",
    "\n",
    "The experiment-specific configuration file should be in the following format:\n",
    "\n",
    "```yaml\n",
    "dataset_name_or_id: \"<DATASET_NAME_OR_ID>\"\n",
    "experiment_name: \"<EXPERIMENT_NAME>\"\n",
    "tracking_uri: \"<TRACKING_URI>\"\n",
    "mlflow_token: \"<MLFLOW_TOKEN>\"\n",
    "nnunet_trainer: \"<NNUNET_TRAINER>[OPTIONAL]\"\n",
    "```\n",
    "`dataset_name_or_id` and `experiment_ame` are used as a reference to the nnUNet Dataset ID and Experiment Name, respectively. These values are used to identify the nnUNet experiment in the nnUNet framework. `Experiment Name` is also used to identify the experiment in the MLFlow server, and to generate the zipped nnUNet model file (as `<Experiment Name>.zip`).\n",
    "\n",
    "\n",
    "`mlflow_token` and `tracking_uri` are used to connect to the MLFlow server, where the experiments are logged, and the trained models are uploaded.\n",
    "\n",
    "`nnunet_trainer` is an optional parameter that specifies the nnUNet trainer to be used for training. If this parameter is not specified, the default nnUNet trainer will be used.\n",
    "\n",
    "\n",
    "### Prepare the Job Configuration Files\n",
    "\n",
    "To prepare the job configuration files, run the following command:\n",
    "\n",
    "```python\n",
    "monai.nvflare.nvflare_generate_job_configs.generate_configs(client_files, experiment_file, script_dir, job_dir)\n",
    "```\n",
    "where:\n",
    "- `<CLIENT_CONFIG_FILE_1>`, `<CLIENT_CONFIG_FILE_2>`, ... are the client-specific configuration files for each client.\n",
    "- `<experiment_file>` is the experiment-specific configuration file.\n",
    "- `<script_dir>` is the directory where the job scripts and python files are stored.\n",
    "- `<job_dir>` is the directory where the job configuration files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    " \n",
    "ROOT_FOLDER = \"/home/maia-user/shared\"\n",
    "sys.path.append(ROOT_FOLDER)\n",
    "\n",
    "Path(ROOT_FOLDER).joinpath(\"Experiments\").mkdir(parents=True, exist_ok=True)\n",
    "Path(ROOT_FOLDER).joinpath(\"Clients\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Experiments/Lymphoma.yaml\n",
    "dataset_name_or_id: \"<EXPERIMENT_ID>\"\n",
    "experiment_name: \"<EXPERIMENT_NAME>\"\n",
    "tracking_uri: \"<MLFLOW_URI>\"\n",
    "nnunet_trainer: \"nnUNetTrainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Clients/site-1.yaml\n",
    "data_dir: \"<DATASET_SITE_1_FOLDER>\"\n",
    "modality_dict:\n",
    "  ct: \".nii.gz\"\n",
    "  label: \".nii.gz\"\n",
    "dataset_format: \"decathlon\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"<nnunet_root_folder>\"\n",
    "client_name: \"site-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Clients/site-2.yaml\n",
    "data_dir: \"<DATASET_SITE_2_FOLDER>\"\n",
    "modality_dict:\n",
    "  ct: \"_CT.nii.gz\"\n",
    "  label: \"_label.nii.gz\"\n",
    "dataset_format: \"subfolders\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"<nnunet_root_folder>\"\n",
    "client_name: \"site-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/src/Lymphoma/requirements.txt\n",
    "\n",
    "fire\n",
    "pytorch-ignite==0.4.11\n",
    "pydicom==2.4.4\n",
    "minio\n",
    "numpy==1.24.0\n",
    "pandas\n",
    "mlflow\n",
    "monai-nvflare==0.2.4\n",
    "odict\n",
    "pyhocon\n",
    "monai[all]\n",
    "git+https://github.com/SimoneBendazzoli93/MONAI.git@dev\n",
    "git+https://github.com/SimoneBendazzoli93/nnUNet.git\n",
    "#git network_architectures\n",
    "#nnunetv2==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"Spleen\"\n",
    "clients = [\n",
    "    \"site-1\",\n",
    "    \"site-2\"\n",
    "]\n",
    "\n",
    "Path(ROOT_FOLDER).joinpath(\"src\").joinpath(experiment).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install MONAI with NVFlare Support\n",
    "\n",
    "To install MONAI with NVFlare support, run the following command:\n",
    "\n",
    "```python\n",
    "!pip install git+https://github.com/SimoneBendazzoli93/MONAI.git@dev\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install git+https://github.com/SimoneBendazzoli93/MONAI.git@dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.nvflare.nvflare_generate_job_configs import generate_configs\n",
    "\n",
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    #nvflare_exec=\"/home/maia-user/.conda/envs/NVFlare/bin/nvflare\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DIR=str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MONAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
